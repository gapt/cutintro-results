{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "pd.options.display.max_rows = 100\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getDataOld(filename):\n",
    "    data = pd.read_csv(filename, names=['method', 'file', 'status', 'inf_input', 'quant_input', 'cuts_in', 'inf_output', \n",
    "                                        'quant_output', 'termset', 'mingrammar', 'num_mingrammars', 'can_sol', 'min_sol', \n",
    "                                        'time_termset', 'time_dtable', 'time_grammar', 'time_minsol', 'time_prcons', \n",
    "                                        'time_cleanproof'])\n",
    "    data = data.replace(\" \", -1)\n",
    "    data = data.replace(-1, np.nan)\n",
    "    data = data.replace('-1', np.nan)\n",
    "    \n",
    "    # Setting the correct datatypes\n",
    "    columns = set(data.keys())\n",
    "    integer_columns = [c for c in columns if c not in ['method', 'file', 'status']]\n",
    "    for c in integer_columns:\n",
    "        data[c] = data[c].astype(np.float)\n",
    "        \n",
    "    method_map = {\n",
    "        'one_cut_one_quant': '1_dtable',\n",
    "        'one_cut_many_quants': 'many_dtable',\n",
    "        'many_cuts_one_quant_1': '1_maxsat',\n",
    "        'many_cuts_one_quant_2': '1_1_maxsat',\n",
    "    }\n",
    "    data['method'] = data['method'].astype(str).map(method_map)\n",
    "    data['file'] = data['file'].astype(str)\n",
    "    data['status'] = data['status'].astype(str)\n",
    "    \n",
    "    # Adding a column for the database\n",
    "    def getDB (s):\n",
    "        if 'ExampleProof' in s:\n",
    "            return 0\n",
    "        elif 'testing/TSTP' in s:\n",
    "            return 1\n",
    "        elif 'testing/veriT' in s:\n",
    "            return 2\n",
    "        else:\n",
    "            return -1\n",
    "    data['db'] = data.file.apply(getDB)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def getData(filename):\n",
    "    data = pd.read_json(filename)\n",
    "    data = data.sort(['file', 'method'])\n",
    "    \n",
    "    # Adding a column for the database\n",
    "    def getDB (s):\n",
    "        if s.endswith(')'):\n",
    "            return 0\n",
    "        elif 'testing/TSTP/prover9' in s:\n",
    "            return 1\n",
    "        elif 'testing/veriT' in s:\n",
    "            return 2\n",
    "        elif 'testing/TSTP/leanCoP' in s:\n",
    "            return 3\n",
    "        else:\n",
    "            return -1\n",
    "    data['db'] = data.file.apply(getDB)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = \"2015-09-25/results.json\"\n",
    "data = getData(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Other exceptions (aka bugs to hunt)\n",
    "\n",
    "data_other_exp = data[data.status == 'cutintro_other_exception']\n",
    "data_other_exp[['method', 'file', 'status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test that fail for 1_dtable and are ok in 1_maxsat\n",
    "\n",
    "merged = pd.merge(\n",
    "    data[data.method == '1_dtable'],\n",
    "    data[data.method == '1_maxsat'],\n",
    "    on=['file'], suffixes=('_dtable', '_maxsat'))\n",
    "\n",
    "merged[(merged.status_maxsat == 'ok') & (merged.status_dtable != 'ok')][['file', 'status_dtable']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test that are ok in 1_dtable and fail for 1_maxsat\n",
    "\n",
    "merged = pd.merge(\n",
    "    data[data.method == '1_dtable'],\n",
    "    data[data.method == '1_maxsat'],\n",
    "    on=['file'], suffixes=('_dtable', '_maxsat'))\n",
    "\n",
    "merged[(merged.status_maxsat != 'ok') & (merged.status_dtable == 'ok')][['file', 'status_maxsat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Regressions compared to previous test\n",
    "\n",
    "previous_filename = \"2015-03-15/CutIntroDataLog.txt\"\n",
    "previous_data = getDataOld(previous_filename)\n",
    "\n",
    "merged = pd.merge(data, previous_data, on=['method', 'file'], suffixes=('_new','_old'))\n",
    "\n",
    "regression = merged[(merged.status_old == 'ok') & (merged.status_new != 'ok')]\n",
    "\n",
    "regression_delta = regression[(regression.method == '1_dtable') | (regression.method == 'many_dtable')]\n",
    "regression_maxsat = regression[((regression.method == '1_maxsat') | (regression.method == '1_1_maxsat')) ]\n",
    "\n",
    "regression[['method', 'file', 'status_old', 'status_new']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "methods = sorted(set(data.method.dropna()))\n",
    "\n",
    "dbs = {\n",
    "    0: 'Proof examples',\n",
    "    1: 'Prover9',\n",
    "    2: 'VeriT',\n",
    "    3: 'LeanCoP',\n",
    "}\n",
    "\n",
    "dbs = { i: dbs[i] for i in dbs.keys() if i in data.db.values }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"=== Number of tests per database and method:\\n\")\n",
    "\n",
    "for dbi, dbname in dbs.items():\n",
    "    for method in methods:\n",
    "        print(\"{0} - {1}: {2}\".format(dbname, method,\n",
    "                                     data[(data.method==method) & (data.db==dbi)].shape[0]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states = set(data.status.values)\n",
    "status_colors = {}\n",
    "for s in data.status.values:\n",
    "    if s == 'ok':\n",
    "        c = 'green'\n",
    "    elif s == 'cutintro_uncompressible':\n",
    "        c = 'yellow'\n",
    "    elif s == 'cutintro_termset_trivial':\n",
    "        c = 'gold'\n",
    "    elif s.endswith('timeout'):\n",
    "        c = plt.cm.Blues(hash(s)/2**65+0.5)\n",
    "    elif s.endswith('exception'):\n",
    "        c = plt.cm.Reds(hash(s)/2**65+0.5)\n",
    "    else:\n",
    "        c = plt.cm.Greys(hash(s)/2**65+0.5)\n",
    "    status_colors[s] = c\n",
    "\n",
    "def plot_status(d, legend=True, **kwargs):\n",
    "    plt.axis('equal')\n",
    "    d.plot(kind='pie', autopct='%1.1f%%',\n",
    "                        colors=[status_colors[s] for s in d.index], **kwargs) \\\n",
    "        .set_ylabel('')\n",
    "    if legend: plt.legend(d.index, loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_status(data.groupby('status').size(), figsize=(14,12), title='Return status: full set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for dbi, db in dbs.items():\n",
    "    for method in methods:\n",
    "        subdata = data[(data.db==dbi) & (data.method==method)].groupby('status').size()\n",
    "        plot_status(subdata, title='{0}\\n{1}'.format(db,method), labels=None, figsize=(8,6))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timecols = ['time_grammar', 'time_minsol', 'time_prcons', 'time_cleanproof']\n",
    "plt.axis('equal')\n",
    "data[timecols].mean().plot(kind='pie', figsize=(12,12), autopct='%1.1f%%',\n",
    "                           title='Time: full set', legend=True).set_ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xsize, ysize = len(dbs.items()), len(methods)\n",
    "plt.figure(1, figsize=(4*xsize,4*ysize))\n",
    "plt.suptitle('Time consumption per method and db', fontsize=25)\n",
    "for i, method in enumerate(methods):\n",
    "    for j, (dbi, db) in enumerate(dbs.items()):\n",
    "        plt.subplot(ysize, xsize, i*xsize + j + 1)\n",
    "        plt.axis('equal')\n",
    "        data[(data.db==dbi) & (data.method==method)][timecols].mean() \\\n",
    "            .plot(kind='pie', autopct='%1.1f%%', labels=None,\n",
    "                  title='{0}\\n{1}'.format(db,method)).set_ylabel('')\n",
    "plt.legend(timecols, loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xsize, ysize = len(dbs.items()), len(methods)\n",
    "plt.figure(1, figsize=(4*xsize,4*ysize))\n",
    "plt.suptitle('Time consumption per method and db, state==ok', fontsize=25)\n",
    "for i, method in enumerate(methods):\n",
    "    for j, (dbi, db) in enumerate(dbs.items()):\n",
    "        plt.subplot(ysize, xsize, i*xsize + j + 1)\n",
    "        plt.axis('equal')\n",
    "        ax = data[(data.db==dbi) & (data.method==method) & (data.status=='ok')][timecols].mean() \\\n",
    "            .plot(kind='pie', autopct='%1.1f%%', labels=None,\n",
    "                  title='{0}\\n{1}'.format(db,method))\n",
    "        ax.set_ylabel('')\n",
    "plt.legend(timecols, loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Different grammar sizes between different methods: one cut with one quantifier\n",
    "data_dtable = data[(data.method == '1_dtable') & data.grammar_size]\n",
    "data_maxsat = data[(data.method == '1_maxsat') & data.grammar_size]\n",
    "\n",
    "merged = pd.merge(data_dtable, data_maxsat, on=['file'], suffixes=('_dtable', '_maxsat'))\n",
    "merged[merged.grammar_size_dtable != merged.grammar_size_maxsat] \\\n",
    "    [['file', 'grammar_size_dtable', 'grammar_size_maxsat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Different grammar sizes between different methods: one cut with multiple quantifiers\n",
    "data_dtable = data[(data.method == 'many_dtable') & data.grammar_size]\n",
    "data_maxsat = data[(data.method == '2_maxsat') & data.grammar_size]\n",
    "\n",
    "merged = pd.merge(data_dtable, data_maxsat, on=['file'], suffixes=('_dtable', '_maxsat'))\n",
    "merged[merged.grammar_size_dtable != merged.grammar_size_maxsat] \\\n",
    "    [['file', 'grammar_size_dtable', 'grammar_size_maxsat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Better compression with 2x2 quantifiers than 1x2 quantifiers or 2x1 quantifiers\n",
    "data_1x2 = data[(data.method == '2_maxsat') & data.grammar_size]\n",
    "data_2x1 = data[(data.method == '1_1_maxsat') & data.grammar_size]\n",
    "data_2x2 = data[(data.method == '2_2_maxsat') & data.grammar_size]\n",
    "\n",
    "merged = data_1x2.merge(data_2x1, on='file', suffixes=('_1x2', '_2x1')) \\\n",
    "    .merge(data_2x2, on='file')\n",
    "merged[(merged.grammar_size < merged.grammar_size_1x2) & (merged.grammar_size < merged.grammar_size_2x1)] \\\n",
    "    [['file', 'grammar_size', 'grammar_size_2x1', 'grammar_size_1x2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['is_smaller'] = (data.status=='ok') & (data.ehs_resinf < data.resinf_input)\n",
    "data['is_larger'] = (data.status=='ok') & (data.ehs_resinf >= data.resinf_input)\n",
    "data['is_trivial'] = data.status.str.endswith('termset_trivial')\n",
    "data['is_uncompressible'] = data.status.str.endswith('uncompressible')\n",
    "data['is_timeout'] = data.status.str.endswith('timeout')\n",
    "data['is_error'] = ~data.is_smaller & ~data.is_larger & ~data.is_trivial & ~data.is_uncompressible & ~data.is_timeout\n",
    "\n",
    "data[(data.termset<65)&(data.db==1)].groupby('termset').mean() \\\n",
    "    [['is_trivial', 'is_uncompressible', 'is_larger', 'is_smaller', 'is_timeout', 'is_error']] \\\n",
    "    .plot(kind='area', figsize=(16,12), ylim=(0,1), title='Status by termset size (prover9)',\n",
    "          color=['gold', 'yellow', 'green', 'lime', 'royalblue', 'tomato'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Proofs with good compression of EHS/HS\n",
    "data[data.ehs_resinf/data.hs_resinf < 0.1] \\\n",
    "    [['file', 'method', 'ehs_resinf', 'hs_resinf', 'resinf_input']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Proofs with good compression of EHS/input\n",
    "data[data.ehs_resinf/data.resinf_input < 0.2] \\\n",
    "    [['file', 'method', 'ehs_resinf', 'hs_resinf', 'resinf_input']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Proofs with blowup\n",
    "data[data.ehs_resinf > 5*data.hs_resinf] \\\n",
    "    [['file', 'method', 'ehs_resinf', 'hs_resinf', 'resinf_input']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = (data.time_maxsat/data.time_grammar).dropna().plot(kind='hist', figsize=(14,8),\n",
    "    title='Percentage of grammar finding time spent in MaxSAT solver', bins=20)\n",
    "ax.set_xlabel('percentage')\n",
    "ax.set_ylabel('number of testcases')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
